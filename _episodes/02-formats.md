---
title: "Most common Data Formats"
teaching: 0
exercises: 0
questions:
- "What are the most common data formats in Environmental Sciences?"
- "What are the most common python packages to read/write netCDF and HDF data files?"
objectives:
- "Learn to recognize the most common data formats in the ocean, weather and climate sciences."
- "Learn to open, read and write netCDF and HDF files"
keypoints:
- "netCDF, HDF data formats"
---

- In this lesson we will first review the most common data formats used in meteorology and oceanography i.e. 
[netCDF](#netcdf) and [HDF](#hdf). 

- Then we will learn how to recognize data coded in netCDF and HDF formats. 

- Finally, we will learn to read and write files coded in these formats in python. 

We divided this chapter in 2 sections based on the data format to explore:

1. [netCDF data format](#netcdf)
2. [HDF data format](#hdf)


## netCDF

[NetCDF](http://www.unidata.ucar.edu/software/netcdf/docs/netcdf_introduction.html) (network Common Data Form) is:

- Self-Describing 
- Portable 
- Scalable
- Appendable 
- Sharable
- Archivable

 a machine-independent, 
self-describing, binary data format standard for exchanging scientific data. 
The project homepage is hosted by the Unidata program at the University Corporation for Atmospheric Research ([UCAR](http://www.unidata.ucar.edu)). 

### Self-describing?

Information describing the data contents of the file are embedded within the data file itself.

This means that there is a header which describes the layout of the rest of the file, in particular the data arrays, as well as arbitrary file metadata 
in the form of name/value attributes.
As all self-describing data formats, netCDF includes a standard API (Application program interface) and portable data access libraries in a variety of 
languages, including python. There are netCDF tools that can open and work with arbitrary netCDF files, using the embedded descriptions to interpret the data.


What does it really mean when we say that a netCDF file is self-describing? Let's find out with an example "MITgcm_state.nc".
If you do not have downloaded the metos_python data for this lesson, please see [the setup instructions]({{ page.root }}/setup/)).


In this example, we will be looking at output generated by a Unidata tool called 
[ncdump](https://www.unidata.ucar.edu/software/netcdf/netcdf-4/newdocs/netcdf/ncdump.html).
 

~~~
$ ncdump -h metos_python/data/MITgcm_state.nc
~~~
{: .bash}


~~~
netcdf MITgcm_state {
dimensions:
	T = UNLIMITED ; // (3 currently)
	Xp1 = 91 ;
	Y = 40 ;
	Z = 20 ;
	X = 90 ;
	Yp1 = 41 ;
	Zl = 20 ;
variables:
	double Xp1(Xp1) ;
		Xp1:long_name = "longitude of cell corner" ;
		Xp1:units = "degrees_east" ;
	double Y(Y) ;
		Y:long_name = "latitude of cell center" ;
		Y:units = "degrees_north" ;
	double Z(Z) ;
		Z:long_name = "vertical coordinate of cell center" ;
		Z:units = "meters" ;
		Z:positive = "up" ;
	double X(X) ;
		X:long_name = "longitude of cell center" ;
		X:units = "degrees_east" ;
	double Yp1(Yp1) ;
		Yp1:long_name = "latitude of cell corner" ;
		Yp1:units = "degrees_north" ;
	double Zl(Zl) ;
		Zl:long_name = "vertical coordinate of upper cell interface" ;
		Zl:units = "meters" ;
		Zl:positive = "up" ;
	double T(T) ;
		T:long_name = "model_time" ;
		T:units = "s" ;
	int iter(T) ;
		iter:long_name = "iteration_count" ;
	float U(T, Z, Y, Xp1) ;
		U:units = "m/s" ;
		U:coordinates = "XU YU RC iter" ;
	float V(T, Z, Yp1, X) ;
		V:units = "m/s" ;
		V:coordinates = "XV YV RC iter" ;
	float Temp(T, Z, Y, X) ;
		Temp:units = "degC" ;
		Temp:long_name = "potential_temperature" ;
		Temp:coordinates = "XC YC RC iter" ;
	float S(T, Z, Y, X) ;
		S:long_name = "salinity" ;
		S:coordinates = "XC YC RC iter" ;
	float Eta(T, Y, X) ;
		Eta:long_name = "free-surface_r-anomaly" ;
		Eta:units = "m" ;
		Eta:coordinates = "XC YC iter" ;
	float W(T, Zl, Y, X) ;
		W:units = "m/s" ;
		W:coordinates = "XC YC RC iter" ;

// global attributes:
		:MITgcm_version = "checkpoint65m" ;
		:build_user = "annefou" ;
		:build_host = "login-0-1.local" ;
		:build_date = "Wed Jul  1 14:49:29 CEST 2015" ;
		:MITgcm_URL = "http://mitgcm.org" ;
		:MITgcm_tag_id = "1.2166 2015/06/25" ;
		:MITgcm_mnc_ver = 0.9 ;
		:sNx = 45 ;
		:sNy = 20 ;
		:OLx = 2 ;
		:OLy = 2 ;
		:nSx = 2 ;
		:nSy = 2 ;
		:nPx = 1 ;
		:nPy = 1 ;
		:Nx = 90 ;
		:Ny = 40 ;
		:Nr = 20 ;
}

~~~
{: .output}

### Portable?

A NetCDF file is machine-independent i.e. it can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.
This is not the case of many binary formats for 
which additional information on the internal data representation must be known. For instance,
issues such as [endianness](https://en.wikipedia.org/wiki/Endianness) being addressed in the software libraries. 

### Scalable?

A small subset of a large dataset may be accessed efficiently.

### Appendable?
Data may be appended to a properly structured NetCDF file without copying the dataset or redefining its structure.

### Sharable?
One writer and multiple readers may simultaneously access the same NetCDF file.

### Archivable? 
Access to all earlier forms of NetCDF data will be supported by current and future versions of the software.


But all these native characteristics are often insufficient...


### Check a netCDF file?

Most of the time, netCDF filename extensions are either .nc or .nc4. However, it is not a mandatory requirement so it is useful to 
learn to check if a file is a netCDF file:
 
 
~~~
$ cd metos_python/data
$ file MITgcm_state.nc
~~~
{: .bash}
 
~~~
MITgcm_state.nc: NetCDF Data Format data
~~~
{: .output} 

[file](https://en.wikipedia.org/wiki/File_(command)) is a bash shell command and is not a netCDF utility. You can use it for any 
kind of files and it attempts torecognize its data format.


### netCDF and python?

The most common low-level python packages to handle netcdf is called netcdf4 python package. The functionalities covered by this python package are close to
those covered by Unidata netCDF library.

Start a new jupyter notebook and enter:

~~~
import netCDF4
~~~
{: .python}



> ### Types of netCDF files?
>
> There are four NetCDF format variants according to the [Unidata NetCDF FAQ page](http://www.unidata.ucar.edu/software/netcdf/docs/faq.html#fv1):
>
> - the classic format,
> - the 64-bit offset format,
> - the NetCDF-4 format, and
> - the NetCDF-4 classic model format. While this seems to add even more complexity to using NetCDF files, the reality is that unless you are generating NetCDF files, most applications read NetCDF files regardless of type with no issues. This aspect has been abstracted for the general user!
>
> The classic format has its roots in the original version of the NetCDF standard. It is the default for new files.
> 
> The 64-bit offset simply allows for larger dataset to be created. Prior to the offset, files would be limited to 2 GB. 
> A 64-bit machine is not required to read a 64-bit file. This point should not be a concern for many users.
> 
> The NetCDF-4 format adds many new features related to compression and multiple unlimited dimensions. NetCDF-4 is essentially a special case of the [HDF5](#hdf) 
> file format. netCDF4 is and extension to the classic model often called netCDF3. netCDF4 adds more powerful forms of data representation and data types at 
> the expense of some additional complexity; it is based on [HDF](#hdf) and therefore requires the installation of the HDF libraries prior to 
> the installation of netCDF. It you installed netCDF without having HDF libraries on your machine, then you probably only have netCDF3.
> 
> The NetCDF-4 classic model format attempts to bridge gaps between the original NetCDF file and NetCDF-4.
> 
> Luckily for us, the [NetCDF4 Python module](https://github.com/Unidata/netcdf4-python) handles many of these differences. 
{: .callout}


### Creating netCDF files

All of the following code examples assume that the netcdf4-python library has been imported:

~~~
import netCDF4
~~~
{: .python}

Then 4 following steps:

1. [Dataset and Files](#create_an_empty_netcdf4_dataset_and_store_it_on_disk)
2. [Dimensions](#dimensions)
3. [Variables](#variables)
4. [Writing and Retrieving data](#writing_and_retrieving_data)


#### Create an empty netCDF4 dataset and store it on disk

~~~
import netCDF4
foo = netCDF4.Dataset('foo.nc', 'w')
foo.close()
~~~
{: .python}

Then you should find a new file called "foo.nc" in your working directory:

~~~
$ ls
~~~
{: .bash}

~~~
$ foo.nc
~~~
{: .output}

The Dataset constructor defaults to creating NETCDF4 format objects. Other formats may be specified with the format keyword argument 
(see the [netCDF4-python docs](http://unidata.github.io/netcdf4-python/)).

The first argument that Dataset takes is the path and name of the netCDF4 file that will be created, updated, or read. 
The second argument is the mode with which to access the file. Use:

- w (write mode) to create a new file, use clobber=True to over-write and existing one
- r (read mode) to open an existing file read-only
- r+ (append mode) to open an existing file and change its contents

#### Dimensions

Create dimensions on a dataset with the createDimension() method. We first create a new netCDF file:

~~~
import netCDF4
f = netCDF4.Dataset('orography.nc', 'w')
~~~
{: .python}

But this time we don't close it immediately and create 3 dimensions:


~~~
f.createDimension('time', None)
f.createDimension('z', 3)
f.createDimension('y', 4)
f.createDimension('x', 5)
~~~
{: .python}

The first dimension is called time with unlimited size (i.e. variable values may be appended along the this dimension). 
Unlimited size dimensions must be declared before (“to the left of”) other dimensions. We usually use only a single unlimited size dimension 
that is used for time.

The other 3 dimensions are spatial dimensions with sizes of 3, 4, and 5, respectively.

The recommended maximum number of dimensions is 4. The recommended order of dimensions is time, z, y, x. 
Not all datasets are required to have all 4 dimensions.

#### Variables

Create variables on a dataset with the createVariable() method, for example:
~~~
lats = f.createVariable('lat', float, ('y', ), zlib=True)
lons = f.createVariable('lon', float, ('x', ), zlib=True)
orography = f.createVariable('orog', float, ('y', 'x'), zlib=True, least_significant_digit=1, fill_value=0)
~~~
{: .python}

The first argument to createVariable() is the variable name. 

The second argument is the variable type. There are many way of specifying type, but Python built-in types work well in the absence of specific requirements.

The third argument is a tuple of previously defined dimension names. As noted above,

The recommended maximum number of dimensions is 4
The recommended order of dimensions is t, z, y, x
Not all variables are required to have all 4 dimensions
All variables should be created with the zlib=True argument to enable data compression within the netCDF4 file.

When appropriate, the least_significant_digit argument should be used to improve compression and storage efficiency by quantizing the variable data 
to the specified precision. In the example above the orography data will be quantized such that a precision of 0.1 is retained.

When appropriate, the fill_value argument can be used to specify the value that the variable gets filled with before any data is written to it. 
Doing so overrides the default netCDF _FillValue (which depends on the type of the variable). If fill_value is set to False, then the variable is not pre-filled.
 
In the example above the orography data will be initialized to zero, the appropriate value for grid points that are over ocean.

#### Writing and Retrieving Data

##### Writing Data

Variable data in netCDF4 datasets are stored in NumPy array or masked array objects.

An appropriately sized and shaped NumPy array can be loaded into a dataset variable by assigning it to a slice that span the variable:


~~~
# create latitude and longitude 1D arrays
lat_out  = [60, 65, 70, 75]
lon_out  = [ 30,  60,  90, 120, 150]
# Create field values for orography
data_out = np.arange(4*5) # 1d array but with dimension x*y
data_out.shape = (4,5)    # reshape to 2d array
orography[:] = data_out
lats[:] = lat_out
lons[:] = lon_out
# close file to write on disk
f.close()
~~~
{: .python}

We defined a one-dimensional array data_out and reshape it to a 2 dimensional array and then store it in the orography netCDF variable.

Let's have a close look to the netCDF file we generated:
~~~
$ ncdump orography.nc
~~~
{: .bash}

~~~
netcdf orography {
dimensions:
	time = UNLIMITED ; // (0 currently)
	z = 3 ;
	y = 4 ;
	x = 5 ;
variables:
	double lat(y) ;
	double lon(x) ;
	double orog(y, x) ;
		orog:_FillValue = 0. ;
		orog:least_significant_digit = 1LL ;
data:

 lat = 60, 65, 70, 75 ;

 lon = 30, 60, 90, 120, 150 ;

 orog =
  _, 1, 2, 3, 4,
  5, 6, 7, 8, 9,
  10, 11, 12, 13, 14,
  15, 16, 17, 18, 19 ;
}

~~~
{: .output}

> ## Understand netCDF file
>
> Check the orography.nc file:
>
> 1. What is the first value of the orog variable?
> 2. What is the missing value for lat and lon variables?
>
> > ## Solution
> > 1. The first value of orog is 0 but it appears as "_" which 
> > corresponds to a missing value (also called filled value).
> > When we defined orog variable, we set its filled/missing value to 0. 
> > Therefore, every occurence of 0 will be flagged as a missing value.
> > 2. We haven't specified any filled values for lat and lon so default
> >    missing values are set by default, depending on the type of variable; 
> >    here lat and lon were defined as float so the default missing value is
> >    9.969209968386869e+36.
> >    You can check the netCDF4 default filled values:
> > `netCDF4.default_fillvals`
> {: .solution}
{: .challenge}

##### Retrieve Data


Let's first read the file we previously generated:

~~~
import netCDF4
import numpy as np

f = netCDF4.Dataset('orography.nc', 'r')
lats = f.variables['lat']
lons = f.variables['lon']
orography = f.variables['orog']
print(lats[:])
print(lons[:])
print(orography[:])
f.close()
~~~
{: .python}

> ## lats vs. lats[:]?
>
> 1. What is the type of `lats`?
> 2. What is the difference between `lats` and `lats[:]`?
>
> > ## Solution
> > 1. lats is a netCDF variable:
> > ` type(lats)`
> > returns:
> > `<class 'netCDF4._netCDF4.Variable'>`
> >
> > 2. lats is a netCDF variable; a lot more than a simple numpy array while lats[:] 
> >    allows you to access the latitudes values stored in the lats netCDF variable. lats[:] is a numpy array.
> >
> {: .solution}
{: .challenge}


Values can be retrieved using most of the usual NumPy indexing and slicing techniques.

However, there are differences between the NumPy and netCDF variable slicing rules; see the 
[netCDF4-python](http://unidata.github.io/netcdf4-python/) docs for details.

Now if we only want to access a subset of the variable orog:
~~~
import netCDF4
import numpy as np

f = netCDF4.Dataset('orography.nc', 'r')
lats = f.variables['lat']
lons = f.variables['lon']
orography = f.variables['orog']
print(lats[:])
print(lons[:])
print(orography[:][3,2])
f.close()
~~~
{: .python}

### CF conventions?

netCDF format is self-describing but very flexible and you still have to decide how to encode your data into the format:

- Layout of data within the file
- Unambiguous names for fields; Use standard names if possible
- Units
- Fill/missing values

Therefore to simplify developments of tools and speed-up netCDF data processing, metadata standards for netCDF files have been created. 
The most common in our discipline is the [Climate and Forecast metadata standard](http://cfconventions.org/), also called CF conventions.


The CF conventions have been adopted by the Program for Climate Model Diagnosis and Intercomparison ([PCMDI](http://www-pcmdi.llnl.gov/)), 
the Earth System Modeling Framework ([ESMF](https://www.earthsystemcog.org/projects/esmf/)), [NCAR](https://ncar.ucar.edu/), and various EU and 
international projects. 

If you plan to create netCDF files, following CF conventions is recommended. However, if you are curious or encounter data using a different convention, 
Unidata maintains [a list](http://www.unidata.ucar.edu/software/netcdf/conventions.html) you can use to find out more information. 


Let's have a look at a netCDF file called `sresa1b_ncar_ccsm3-example.nc` that follows CF conventions.
If you haven't downloaded this file, see [the setup instructions]({{ page.root }}/setup/) or download it now:

~~~
curl https://www.unidata.ucar.edu/software/netcdf/examples/sresa1b_ncar_ccsm3-example.nc -o sresa1b_ncar_ccsm3-example.nc
{: .bash}

~~~
$ ncdump -h metos_python/data/sresa1b_ncar_ccsm3-example.nc
~~~


~~~
netcdf sresa1b_ncar_ccsm3-example {
dimensions:
	lat = 128 ;
	lon = 256 ;
	bnds = 2 ;
	plev = 17 ;
	time = UNLIMITED ; // (1 currently)

variables:
	float area(lat, lon) ;
		area:long_name = "Surface area" ;
		area:units = "meter2" ;
	float lat(lat) ;
		lat:long_name = "latitude" ;
		lat:units = "degrees_north" ;
		lat:axis = "Y" ;
		lat:standard_name = "latitude" ;
		lat:bounds = "lat_bnds" ;
	double lat_bnds(lat, bnds) ;
	float lon(lon) ;
		lon:long_name = "longitude" ;
		lon:units = "degrees_east" ;
		lon:axis = "X" ;
		lon:standard_name = "longitude" ;
		lon:bounds = "lon_bnds" ;
	double lon_bnds(lon, bnds) ;
	long msk_rgn(lat, lon) ;
		msk_rgn:long_name = "Mask region" ;
		msk_rgn:units = "bool" ;
	double plev(plev) ;
		plev:long_name = "pressure" ;
		plev:units = "Pa" ;
		plev:standard_name = "air_pressure" ;
		plev:positive = "down" ;
		plev:axis = "Z" ;
	float pr(time, lat, lon) ;
		pr:comment = "Created using NCL code CCSM_atmm_2cf.ncl on\n",
    " machine eagle163s" ;
		pr:missing_value = 1.e+20f ;
		pr:_FillValue = 1.e+20f ;
		pr:cell_methods = "time: mean (interval: 1 month)" ;
		pr:history = "(PRECC+PRECL)*r[h2o]" ;
		pr:original_units = "m-1 s-1" ;
		pr:original_name = "PRECC, PRECL" ;
		pr:standard_name = "precipitation_flux" ;
		pr:units = "kg m-2 s-1" ;
		pr:long_name = "precipitation_flux" ;
		pr:cell_method = "time: mean" ;
	float tas(time, lat, lon) ;
		tas:comment = "Created using NCL code CCSM_atmm_2cf.ncl on\n",
    " machine eagle163s" ;
		tas:missing_value = 1.e+20f ;
		tas:_FillValue = 1.e+20f ;
		tas:cell_methods = "time: mean (interval: 1 month)" ;
		tas:history = "Added height coordinate" ;
		tas:coordinates = "height" ;
		tas:original_units = "K" ;
		tas:original_name = "TREFHT" ;
		tas:standard_name = "air_temperature" ;
		tas:units = "K" ;
		tas:long_name = "air_temperature" ;
		tas:cell_method = "time: mean" ;
	double time(time) ;
		time:calendar = "noleap" ;
		time:standard_name = "time" ;
		time:axis = "T" ;
		time:units = "days since 0000-1-1" ;
		time:bounds = "time_bnds" ;
		time:long_name = "time" ;
	double time_bnds(time, bnds) ;
	float ua(time, plev, lat, lon) ;
		ua:comment = "Created using NCL code CCSM_atmm_2cf.ncl on\n",
    " machine eagle163s" ;
		ua:missing_value = 1.e+20f ;
		ua:cell_methods = "time: mean (interval: 1 month)" ;
		ua:long_name = "eastward_wind" ;
		ua:history = "Interpolated U with NCL \'vinth2p_ecmwf\'" ;
		ua:units = "m s-1" ;
		ua:original_units = "m s-1" ;
		ua:original_name = "U" ;
		ua:standard_name = "eastward_wind" ;
		ua:_FillValue = 1.e+20f ;

// global attributes:
		:CVS_Id = "$Id$" ;
		:creation_date = "" ;
		:prg_ID = "Source file unknown Version unknown Date unknown" ;
		:cmd_ln = "bds -x 256 -y 128 -m 23 -o /data/zender/data/dst_T85.nc" ;
		:history = "Tue Oct 25 15:08:51 2005: ncks -O -x -v va -m sresa1b_ncar_ccsm3_0_run1_200001.nc sresa1b_ncar_ccsm3_0_run1_200001.nc\n",
    "Tue Oct 25 15:07:21 2005: ncks -d time,0 sresa1b_ncar_ccsm3_0_run1_200001_201912.nc sresa1b_ncar_ccsm3_0_run1_200001.nc\n",
    "Tue Oct 25 13:29:43 2005: ncks -d time,0,239 sresa1b_ncar_ccsm3_0_run1_200001_209912.nc /var/www/html/tmp/sresa1b_ncar_ccsm3_0_run1_200001_201912.nc\n",
    "Thu Oct 20 10:47:50 2005: ncks -A -v va /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/sresa1b_ncar_ccsm3_0_run1_va_200001_209912.nc /data/brownmc/sresa1b/atm/mo/tas/ncar_ccsm3_0/run1/sresa1b_ncar_ccsm3_0_run1_200001_209912.nc\n",
    "Wed Oct 19 14:55:04 2005: ncks -F -d time,01,1200 /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/sresa1b_ncar_ccsm3_0_run1_va_200001_209912.nc /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/sresa1b_ncar_ccsm3_0_run1_va_200001_209912.nc\n",
    "Wed Oct 19 14:53:28 2005: ncrcat /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/foo_05_1200.nc /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/foo_1192_1196.nc /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/sresa1b_ncar_ccsm3_0_run1_va_200001_209912.nc\n",
    "Wed Oct 19 14:50:38 2005: ncks -F -d time,05,1200 /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/va_A1.SRESA1B_1.CCSM.atmm.2000-01_cat_2099-12.nc /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/foo_05_1200.nc\n",
    "Wed Oct 19 14:49:45 2005: ncrcat /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/va_A1.SRESA1B_1.CCSM.atmm.2000-01_cat_2079-12.nc /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/va_A1.SRESA1B_1.CCSM.atmm.2080-01_cat_2099-12.nc /data/brownmc/sresa1b/atm/mo/va/ncar_ccsm3_0/run1/va_A1.SRESA1B_1.CCSM.atmm.2000-01_cat_2099-12.nc\n",
    "Created from CCSM3 case b30.040a\n",
    " by wgstrand@ucar.edu\n",
    " on Wed Nov 17 14:12:57 EST 2004\n",
    " \n",
    " For all data, added IPCC requested metadata" ;
		:table_id = "Table A1" ;
		:title = "model output prepared for IPCC AR4" ;
		:institution = "NCAR (National Center for Atmospheric \n",
    "Research, Boulder, CO, USA)" ;
		:source = "CCSM3.0, version beta19 (2004): \n",
    "atmosphere: CAM3.0, T85L26;\n",
    "ocean     : POP1.4.3 (modified), gx1v3\n",
    "sea ice   : CSIM5.0, T85;\n",
    "land      : CLM3.0, gx1v3" ;
		:contact = "ccsm@ucar.edu" ;
		:project_id = "IPCC Fourth Assessment" ;
		:Conventions = "CF-1.0" ;
		:references = "Collins, W.D., et al., 2005:\n",
    " The Community Climate System Model, Version 3\n",
    " Journal of Climate\n",
    " \n",
    " Main website: http://www.ccsm.ucar.edu" ;
		:acknowledgment = " Any use of CCSM data should acknowledge the contribution\n",
    " of the CCSM project and CCSM sponsor agencies with the \n",
    " following citation:\n",
    " \'This research uses data provided by the Community Climate\n",
    " System Model project (www.ccsm.ucar.edu), supported by the\n",
    " Directorate for Geosciences of the National Science Foundation\n",
    " and the Office of Biological and Environmental Research of\n",
    " the U.S. Department of Energy.\'\n",
    "In addition, the words \'Community Climate System Model\' and\n",
    " \'CCSM\' should be included as metadata for webpages referencing\n",
    " work using CCSM data or as keywords provided to journal or book\n",
    "publishers of your manuscripts.\n",
    "Users of CCSM data accept the responsibility of emailing\n",
    " citations of publications of research using CCSM data to\n",
    " ccsm@ucar.edu.\n",
    "Any redistribution of CCSM data must include this data\n",
    " acknowledgement statement." ;
		:realization = 1 ;
		:experiment_id = "720 ppm stabilization experiment (SRESA1B)" ;
		:comment = "This simulation was initiated from year 2000 of \n",
    " CCSM3 model run b30.030a and executed on \n",
    " hardware cheetah.ccs.ornl.gov. The input external forcings are\n",
    "ozone forcing    : A1B.ozone.128x64_L18_1991-2100_c040528.nc\n",
    "aerosol optics   : AerosolOptics_c040105.nc\n",
    "aerosol MMR      : AerosolMass_V_128x256_clim_c031022.nc\n",
    "carbon scaling   : carbonscaling_A1B_1990-2100_c040609.nc\n",
    "solar forcing    : Fixed at 1366.5 W m-2\n",
    "GHGs             : ghg_ipcc_A1B_1870-2100_c040521.nc\n",
    "GHG loss rates   : noaamisc.r8.nc\n",
    "volcanic forcing : none\n",
    "DMS emissions    : DMS_emissions_128x256_clim_c040122.nc\n",
    "oxidants         : oxid_128x256_L26_clim_c040112.nc\n",
    "SOx emissions    : SOx_emissions_A1B_128x256_L2_1990-2100_c040608.nc\n",
    " Physical constants used for derived data:\n",
    " Lv (latent heat of evaporation): 2.501e6 J kg-1\n",
    " Lf (latent heat of fusion     ): 3.337e5 J kg-1\n",
    " r[h2o] (density of water      ): 1000 kg m-3\n",
    " g2kg   (grams to kilograms    ): 1000 g kg-1\n",
    " \n",
    " Integrations were performed by NCAR and CRIEPI with support\n",
    " and facilities provided by NSF, DOE, MEXT and ESC/JAMSTEC." ;
		:model_name_english = "NCAR CCSM" ;
}
~~~
{: .output} 
 
## HDF

Hierarchical Data Format ([HDF](https://support.hdfgroup.org/)) is a data file format designed by the National Center for Supercomputing Applications ([NCSA](http://www.ncsa.illinois.edu/)).

It is now developed and maintained by the [HDF group](https://www.hdfgroup.org/).

Hierarchical Data Format, commonly abbreviated HDF, HDF4, or HDF5, is a library and multi-object file format for the transfer of graphical and numerical data between computers. 

HDF supports several different data models, including multidimensional arrays, 
[raster images](https://en.wikipedia.org/wiki/Raster_graphics), and tables. 
Each defines a specific aggregate data type and provides an API for reading, writing, and organising the data and metadata. New data models can be added by the HDF developers or users. 

HDF is self-describing, allowing an application to interpret the structure and contents of a file without any outside information. One HDF file can hold a mixture of related objects, which can be accessed as a group or as individual objects.


#### Read an HDF file


#### Create an HDF file


