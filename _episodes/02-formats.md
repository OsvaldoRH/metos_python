---
title: "Most common Data Formats"
teaching: 0
exercises: 0
questions:
- "What are the most common data formats in the Ocean, Weather and Climate Sciences?"
- "What are the most common python packages to read/write netCDF, HDF, GRIB and BUFR data files?"
objectives:
- "Learn to recognize the most common data formats in the ocean, weather and climate sciences."
- "Learn to open, read and write netCDF, HDF, GRIB and BUFR files"
keypoints:
- "netCDF, HDF, GRIB and BUFR data formats"
---

- In this lesson we will first review the most common data formats used in meteorology and oceanography i.e. 
[netCDF](#netcdf), [HDF](#hdf), WMO [GRIB](#grib) and [BUFR](#bufr). 

- Then we will learn how to recognize data coded in netCDF, HDF, GRIB and BUFR formats. 

- Finally, we will learn to read and write files coded in these formats in python. 


## netCDF

[NetCDF](http://www.unidata.ucar.edu/software/netcdf/docs/netcdf_introduction.html) (network Common Data Form) is:

- Self-Describing 
- Portable 
- Scalable
- Appendable 
- Sharable
- Archivable

 a machine-independent, 
self-describing, binary data format standard for exchanging scientific data. 
The project homepage is hosted by the Unidata program at the University Corporation for Atmospheric Research ([UCAR](http://www.unidata.ucar.edu)). 

### Self-describing?

Information describing the data contents of the file are embedded within the data file itself.

This means that there is a header which describes the layout of the rest of the file, in particular the data arrays, as well as arbitrary file metadata 
in the form of name/value attributes.
As all self-describing data formats, netCDF includes a standard API (Application program interface) and portable data access libraries in a variety of 
languages, including python. There are netCDF tools that can open and work with arbitrary netCDF files, using the embedded descriptions to interpret the data.


What does it really mean when we say that a netCDF file is self-describing? Let's find out with an example "temperature.nc". 

In this example, we will be looking at output generated by a Unidata tool called [ncdump](https://www.unidata.ucar.edu/software/netcdf/netcdf-4/newdocs/netcdf/ncdump.html).
 

~~~

$ ncdump temperature.nc
~~~
{: .bash}


~~~
,,,
~~~
{: .output}

### Portable?

A NetCDF file is machine-independent i.e. it can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.
This is not the case of many binary formats for 
which additional information on the internal data representation must be known. For instance,
issues such as [endianness](https://en.wikipedia.org/wiki/Endianness) being addressed in the software libraries. 

### Scalable?

A small subset of a large dataset may be accessed efficiently.

### Appendable?
Data may be appended to a properly structured NetCDF file without copying the dataset or redefining its structure.

### Sharable?
One writer and multiple readers may simultaneously access the same NetCDF file.

### Archivable? 
Access to all earlier forms of NetCDF data will be supported by current and future versions of the software.


But all these native characteristics are often insufficient...

### CF conventions?

netCDF format is slef-describing but very flexible and you still have to decide how to encode your data into the format:

- Layout of data within the file
- Unambiguous names for fields; Use standard names if possible
- Units
- Fill/missing values

Therefore to simplify developments of tools and speed-up netCDF data processing, metadata standards for netCDF files have been created. 
The most common in our discipline is the [Climate and Forecast metadata standard](http://cfconventions.org/), also called CF conventions.


The CF conventions have been adopted by the Program for Climate Model Diagnosis and Intercomparison ([PCMDI](http://www-pcmdi.llnl.gov/)), 
the Earth System Modeling Framework ([ESMF](https://www.earthsystemcog.org/projects/esmf/)), [NCAR](https://ncar.ucar.edu/), and various EU and 
international projects. 

If you plan to create netCDF files, following CF conventions is recommended. However, if you are curious or encounter data using a different convention, 
Unidata maintains [a list](http://www.unidata.ucar.edu/software/netcdf/conventions.html) you can use to find out more information. 

In this workshop, we will use and generate files that are CF compliant. 

### Check a netCDF file?

Most of the time, netCDF filename extensions are either .nc or .nc4. However, it is not a mandatory requirement so it is useful to 
learn to check if a file is a netCDF file:
 
 
~~~
$ cd metos_python/data
$ file temperature.nc
~~~
{: .bash}
 
~~~
temperature.nc: NetCDF Data Format data
~~~
{: .output} 

[file](https://en.wikipedia.org/wiki/File_(command)) is a bash shell command and is not a netCDF utility. You can use it for any 
kind of files and it attempts torecognize its data format.


### netCDF and python?

The most common low-level python packages to handle netcdf is called netcdf4 python package. The functionalities covered by this python package are close to
those covered by Unidata netCDF library.

Start a new jupyter notebook and enter:

~~~
import netCDF4
~~~
{: .python}


#### Read a netCDF file


#### Create a netCDF file



### Types of netCDF files?

There are four NetCDF format variants according to the [Unidata NetCDF FAQ page](http://www.unidata.ucar.edu/software/netcdf/docs/faq.html#fv1):

- the classic format,
- the 64-bit offset format,
- the NetCDF-4 format, and
- the NetCDF-4 classic model format. While this seems to add even more complexity to using NetCDF files, the reality is that unless you are generating NetCDF files, most applications read NetCDF files regardless of type with no issues. This aspect has been abstracted for the general user!

The classic format has its roots in the original version of the NetCDF standard. It is the default for new files.

The 64-bit offset simply allows for larger dataset to be created. Prior to the offset, files would be limited to 2 GB. 
A 64-bit machine is not required to read a 64-bit file. This point should not be a concern for many users.

The NetCDF-4 format adds many new features related to compression and multiple unlimited dimensions. NetCDF-4 is essentially a special case of the [HDF5](#hdf) 
file format. netCDF4 is and extension to the classic model often called netCDF3. netCDF4 adds more powerful forms of data representation and data types at 
the expense of some additional complexity; it is based on [HDF](#hdf) and therefore requires the installation of the HDF libraries prior to 
the installation of netCDF. It you installed netCDF without having HDF libraries on your machine, then you probably only have netCDF3.

The NetCDF-4 classic model format attempts to bridge gaps between the original NetCDF file and NetCDF-4.

Luckily for us, the [NetCDF4 Python module](https://github.com/Unidata/netcdf4-python) handles many of these differences. 



## HDF

Hierarchical Data Format ([HDF](https://support.hdfgroup.org/)) is a data file format designed by the National Center for Supercomputing Applications ([NCSA](http://www.ncsa.illinois.edu/)).

It is now developed and maintained by the [HDF group](https://www.hdfgroup.org/).

Hierarchical Data Format, commonly abbreviated HDF, HDF4, or HDF5, is a library and multi-object file format for the transfer of graphical and numerical data between computers. 

HDF supports several different data models, including multidimensional arrays, 
[raster images](https://en.wikipedia.org/wiki/Raster_graphics), and tables. 
Each defines a specific aggregate data type and provides an API for reading, writing, and organising the data and metadata. New data models can be added by the HDF developers or users. 

HDF is self-describing, allowing an application to interpret the structure and contents of a file without any outside information. One HDF file can hold a mixture of related objects, which can be accessed as a group or as individual objects.


#### Read an HDF file


#### Create an HDF file

## WMO Binary data exchange formats: GRIB and BUFR

WMO GRIB and BUFR data formats are Table Driven Code Forms which means you need the corresponding "table" to decode GRIB or BUFR data. 

These two formats have been widely adopted for the distribution of meteorological satellite products, especially those processed to level 2 or beyond. 
They are described in the Operational Codes and Manual on Codes. By packing information into the BUFR or GRIB code, data records can be made more compact than many other formats, resulting in faster computer-to-computer transmissions. 

The formats can equally well serve as a data storage formats, generating the same efficiencies relative to information storage and retrieval devices.

Software for encoding and decoding data in the BUFR and GRIB formats is freely available for download from the [ECMWF software](https://software.ecmwf.int/wiki/display/ECC/ecCodes+Home).

### BUFR

The WMO Binary Universal Form for the Representation of meteorological data (BUFR) is a binary code designed to represent any meteorological dataset employing a continuous binary stream. It has been designed to achieve efficient exchange and storage of meteorological and oceanographic data. It is self describing, table driven and very flexible data representation system, especially for huge volumes of data.



#### Read a BUFR file

#### Create a BUFR file


### GRIB 

Similarly, another widely used bit-oriented data exchange scheme is the WMO GRIddedBinary (GRIB) format. GRIB is an efficient vehicle for transmitting large volumes of gridded data to automated centers over high-speed telecommunication lines, using modern protocols. An updated version of GRIB, commonly abbreviated to GRIB-2, is currently being introduced and is most relevant for use with satellite data.


#### Read a GRIB file


#### Create a GRIB file


