---
title: "Most common Data Formats"
teaching: 0
exercises: 0
questions:
- "What are the most common data formats in the Ocean, Weather and Climate Sciences?"
- "What are the most common python packages to read/write netCDF, HDF, GRIB and BUFR data files?"
objectives:
- "Learn to recognize the most common data formats in the ocean, weather and climate sciences."
- "Learn to open, read and write netCDF, HDF, GRIB and BUFR files"
keypoints:
- "netCDF, HDF, GRIB and BUFR data formats"
---

- In this lesson we will first review the most common data formats used in meteorology and oceanography i.e. 
[netCDF](#netcdf), [HDF](#hdf), WMO [GRIB](#grib) and [BUFR](#bufr). 

- Then we will learn how to recognize data coded in netCDF, HDF, GRIB and BUFR formats. 

- Finally, we will learn to read and write files coded in these formats in python. 


## netCDF

[NetCDF](http://www.unidata.ucar.edu/software/netcdf/docs/netcdf_introduction.html) (network Common Data Form) is:

- Self-Describing 
- Portable 
- Scalable
- Appendable 
- Sharable
- Archivable

 a machine-independent, 
self-describing, binary data format standard for exchanging scientific data. 
The project homepage is hosted by the Unidata program at the University Corporation for Atmospheric Research ([UCAR](http://www.unidata.ucar.edu)). 

### Self-describing?

Information describing the data contents of the file are embedded within the data file itself.

This means that there is a header which describes the layout of the rest of the file, in particular the data arrays, as well as arbitrary file metadata 
in the form of name/value attributes.
As all self-describing data formats, netCDF includes a standard API (Application program interface) and portable data access libraries in a variety of 
languages, including python. There are netCDF tools that can open and work with arbitrary netCDF files, using the embedded descriptions to interpret the data.


What does it really mean when we say that a netCDF file is self-describing? Let's find out with an example "temperature.nc". 

In this example, we will be looking at output generated by a Unidata tool called [ncdump](https://www.unidata.ucar.edu/software/netcdf/netcdf-4/newdocs/netcdf/ncdump.html).
 

~~~

$ ncdump temperature.nc
~~~
{: .bash}


~~~
,,,
~~~
{: .output}

### Portable?

A NetCDF file is machine-independent i.e. it can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.
This is not the case of many binary formats for 
which additional information on the internal data representation must be known. For instance,
issues such as [endianness](https://en.wikipedia.org/wiki/Endianness) being addressed in the software libraries. 

### Scalable?

A small subset of a large dataset may be accessed efficiently.

### Appendable?
Data may be appended to a properly structured NetCDF file without copying the dataset or redefining its structure.

### Sharable?
One writer and multiple readers may simultaneously access the same NetCDF file.

### Archivable? 
Access to all earlier forms of NetCDF data will be supported by current and future versions of the software.


But all these native characteristics are often insufficient...


### Check a netCDF file?

Most of the time, netCDF filename extensions are either .nc or .nc4. However, it is not a mandatory requirement so it is useful to 
learn to check if a file is a netCDF file:
 
 
~~~
$ cd metos_python/data
$ file temperature.nc
~~~
{: .bash}
 
~~~
temperature.nc: NetCDF Data Format data
~~~
{: .output} 

[file](https://en.wikipedia.org/wiki/File_(command)) is a bash shell command and is not a netCDF utility. You can use it for any 
kind of files and it attempts torecognize its data format.


### netCDF and python?

The most common low-level python packages to handle netcdf is called netcdf4 python package. The functionalities covered by this python package are close to
those covered by Unidata netCDF library.

Start a new jupyter notebook and enter:

~~~
import netCDF4
~~~
{: .python}


### Types of netCDF files?

There are four NetCDF format variants according to the [Unidata NetCDF FAQ page](http://www.unidata.ucar.edu/software/netcdf/docs/faq.html#fv1):

- the classic format,
- the 64-bit offset format,
- the NetCDF-4 format, and
- the NetCDF-4 classic model format. While this seems to add even more complexity to using NetCDF files, the reality is that unless you are generating NetCDF files, most applications read NetCDF files regardless of type with no issues. This aspect has been abstracted for the general user!

The classic format has its roots in the original version of the NetCDF standard. It is the default for new files.

The 64-bit offset simply allows for larger dataset to be created. Prior to the offset, files would be limited to 2 GB. 
A 64-bit machine is not required to read a 64-bit file. This point should not be a concern for many users.

The NetCDF-4 format adds many new features related to compression and multiple unlimited dimensions. NetCDF-4 is essentially a special case of the [HDF5](#hdf) 
file format. netCDF4 is and extension to the classic model often called netCDF3. netCDF4 adds more powerful forms of data representation and data types at 
the expense of some additional complexity; it is based on [HDF](#hdf) and therefore requires the installation of the HDF libraries prior to 
the installation of netCDF. It you installed netCDF without having HDF libraries on your machine, then you probably only have netCDF3.

The NetCDF-4 classic model format attempts to bridge gaps between the original NetCDF file and NetCDF-4.

Luckily for us, the [NetCDF4 Python module](https://github.com/Unidata/netcdf4-python) handles many of these differences. 

### Creating netCDF files

All of the following code examples assume that the netcdf4-python library has been imported:

~~~
import netCDF4
~~~
{: .python}

Then 4 following steps:

1. [Dataset and Files](#create_an_empty_netcdf4_dataset_and_store_it_on_disk)
2. [Dimensions](#dimensions)
3. [Variables](#variables)
4. [Writing and Retrieving data](#writing_and_retrieving_data)


#### Create an empty netCDF4 dataset and store it on disk

~~~
import netCDF4
foo = netCDF4.Dataset('foo.nc', 'w')
foo.close()
~~~
{: .python}

Then you should find a new file called "foo.nc" in your working directory:

~~~
$ ls
~~~
{: .bash}

~~~
$ foo.nc
~~~
{: .output}

The Dataset constructor defaults to creating NETCDF4 format objects. Other formats may be specified with the format keyword argument 
(see the [netCDF4-python docs](http://unidata.github.io/netcdf4-python/)).

The first argument that Dataset takes is the path and name of the netCDF4 file that will be created, updated, or read. 
The second argument is the mode with which to access the file. Use:

- w (write mode) to create a new file, use clobber=True to over-write and existing one
- r (read mode) to open an existing file read-only
- r+ (append mode) to open an existing file and change its contents

#### Dimensions

Create dimensions on a dataset with the createDimension() method. We first create a new netCDF file:

~~~
import netCDF4
f = netCDF4.Dataset('orography.nc', 'w')
~~~
{: .python}

But this time we don't close it immediately and create 3 dimensions:


~~~
f.createDimension('time', None)
f.createDimension('z', 40)
f.createDimension('y', 400)
f.createDimension('x', 200)
~~~
{: .python}

The first dimension is called time with unlimited size (i.e. variable values may be appended along the this dimension). 
Unlimited size dimensions must be declared before (“to the left of”) other dimensions. We usually use only a single unlimited size dimension 
that is used for time.

The other 3 dimensions are spatial dimensions with sizes of 40, 400, and 200, respectively.

The recommended maximum number of dimensions is 4. The recommended order of dimensions is time, z, y, x. 
Not all datasets are required to have all 4 dimensions.

#### Variables

Create variables on a dataset with the createVariable() method, for example:
~~~
lats = f.createVariable('lat', float, ('y', 'x'), zlib=True)
lons = f.createVariable('lon', float, ('y', 'x'), zlib=True)
orography = f.createVariable('orog', float, ('y', 'x'), zlib=True, least_significant_digit=1, fill_value=0)
~~~
{: .python}

The first argument to createVariable() is the variable name. 

The second argument is the variable type. There are many way of specifying type, but Python built-in types work well in the absence of specific requirements.

The third argument is a tuple of previously defined dimension names. As noted above,

The recommended maximum number of dimensions is 4
The recommended order of dimensions is t, z, y, x
Not all variables are required to have all 4 dimensions
All variables should be created with the zlib=True argument to enable data compression within the netCDF4 file.

When appropriate, the least_significant_digit argument should be used to improve compression and storage efficiency by quantizing the variable data 
to the specified precision. In the example above the orography data will be quantized such that a precision of 0.1 is retained.

When appropriate, the fill_value argument can be used to specify the value that the variable gets filled with before any data is written to it. 
Doing so overrides the default netCDF _FillValue (which depends on the type of the variable). If fill_value is set to False, then the variable is not pre-filled.
 
In the example above the orography data will be initialized to zero, the appropriate value for grid points that are over ocean.

#### Writing and Retrieving Data

Variable data in netCDF4 datasets are stored in NumPy array or masked array objects.

An appropriately sized and shaped NumPy array can be loaded into a dataset variable by assigning it to a slice that span the variable:


~~~
import numpy as np

orography[:] = np.arange(48, 51.1, 0.1)

~~~
{: .python}

and values can be retrieved using most of the usual NumPy indexing and slicing techniques.

There are differences between the NumPy and netCDF variable slicing rules; see the netCDF4-python docs for details.


### CF conventions?

netCDF format is slef-describing but very flexible and you still have to decide how to encode your data into the format:

- Layout of data within the file
- Unambiguous names for fields; Use standard names if possible
- Units
- Fill/missing values

Therefore to simplify developments of tools and speed-up netCDF data processing, metadata standards for netCDF files have been created. 
The most common in our discipline is the [Climate and Forecast metadata standard](http://cfconventions.org/), also called CF conventions.


The CF conventions have been adopted by the Program for Climate Model Diagnosis and Intercomparison ([PCMDI](http://www-pcmdi.llnl.gov/)), 
the Earth System Modeling Framework ([ESMF](https://www.earthsystemcog.org/projects/esmf/)), [NCAR](https://ncar.ucar.edu/), and various EU and 
international projects. 

If you plan to create netCDF files, following CF conventions is recommended. However, if you are curious or encounter data using a different convention, 
Unidata maintains [a list](http://www.unidata.ucar.edu/software/netcdf/conventions.html) you can use to find out more information. 

In this workshop, we will use and generate files that are CF compliant. 

## HDF

Hierarchical Data Format ([HDF](https://support.hdfgroup.org/)) is a data file format designed by the National Center for Supercomputing Applications ([NCSA](http://www.ncsa.illinois.edu/)).

It is now developed and maintained by the [HDF group](https://www.hdfgroup.org/).

Hierarchical Data Format, commonly abbreviated HDF, HDF4, or HDF5, is a library and multi-object file format for the transfer of graphical and numerical data between computers. 

HDF supports several different data models, including multidimensional arrays, 
[raster images](https://en.wikipedia.org/wiki/Raster_graphics), and tables. 
Each defines a specific aggregate data type and provides an API for reading, writing, and organising the data and metadata. New data models can be added by the HDF developers or users. 

HDF is self-describing, allowing an application to interpret the structure and contents of a file without any outside information. One HDF file can hold a mixture of related objects, which can be accessed as a group or as individual objects.


#### Read an HDF file


#### Create an HDF file

## WMO Binary data exchange formats: GRIB and BUFR

WMO GRIB and BUFR data formats are Table Driven Code Forms which means you need the corresponding "table" to decode GRIB or BUFR data. 

These two formats have been widely adopted for the distribution of meteorological satellite products, especially those processed to level 2 or beyond. 
They are described in the Operational Codes and Manual on Codes. By packing information into the BUFR or GRIB code, data records can be made more compact than many other formats, resulting in faster computer-to-computer transmissions. 

The formats can equally well serve as a data storage formats, generating the same efficiencies relative to information storage and retrieval devices.

Software for encoding and decoding data in the BUFR and GRIB formats is freely available for download from the [ECMWF software](https://software.ecmwf.int/wiki/display/ECC/ecCodes+Home).

### BUFR

The WMO Binary Universal Form for the Representation of meteorological data (BUFR) is a binary code designed to represent any meteorological dataset employing a continuous binary stream. It has been designed to achieve efficient exchange and storage of meteorological and oceanographic data. It is self describing, table driven and very flexible data representation system, especially for huge volumes of data.



#### Read a BUFR file

#### Create a BUFR file


### GRIB 

Similarly, another widely used bit-oriented data exchange scheme is the WMO GRIddedBinary (GRIB) format. GRIB is an efficient vehicle for transmitting large volumes of gridded data to automated centers over high-speed telecommunication lines, using modern protocols. An updated version of GRIB, commonly abbreviated to GRIB-2, is currently being introduced and is most relevant for use with satellite data.


#### Read a GRIB file


#### Create a GRIB file


